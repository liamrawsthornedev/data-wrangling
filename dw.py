# -*- coding: utf-8 -*-
"""DW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tyNpB94fQzkFEpPfygoEDJ1L-oWzKohZ
"""

!pip install tensorflow==1.14.0

#mount google drive

from google.colab import drive 
drive.mount('/content/gdrive')

#create csv file path

path = "/content/gdrive/My Drive/DWCW/"
mydata = path + "cwdata.csv"
print(mydata)

#create word embedding file path 

we = path + "glove.twitter.27B.100d.txt"
print (we)

#import libraries

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd 
import numpy as np
import scipy as sp

from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.models import Model
from keras.models import Sequential
from keras import layers
from keras.layers.embeddings import Embedding
from keras.layers import Input, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.utils import to_categorical
from keras import metrics

import re
import string

from numpy import argmax

import nltk
nltk.download('punkt')
from nltk import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

#add header row

df = pd.read_csv(mydata, skiprows = 1,header = None)
df.head()

#rename header rows

df.columns = ['ID', 'Message', 'Rating']
df.head()

#assign values to ratings

df.loc[df["Rating"]== "OAG", "Rating"] = 0
df.loc[df["Rating"]== "CAG", "Rating"] = 1
df.loc[df["Rating"]== "NAG", "Rating"] = 2

df.head()

#remove punctuation

def remove_punct(text):
    text_nopunct = ''
    text_nopunct = re.sub('['+string.punctuation+']', '', text)
    return text_nopunct
df['Message_Clean'] = df['Message'].apply(lambda x: remove_punct(x))

del df['Message']
df.head()

#convert messages to lowercase

df = df.applymap(lambda s:s.lower() if type(s) == str else s)

df.head()

#store each column in arrays

ids = df['ID'].values
messages = df['Message_Clean'].values
ratings = df['Rating'].values

newratings = to_categorical(ratings)

sns.countplot(x = 'Rating', data = df)

#train/test
messages_train, messages_test, y_train, y_test = train_test_split(messages, newratings, test_size=0.1)

#tokenize

tokenizer = Tokenizer(num_words=3000)

tokenizer.fit_on_texts(messages_train)

x_train = tokenizer.texts_to_sequences(messages_train)
x_test = tokenizer.texts_to_sequences(messages_test)

words = tokenizer.word_index
vocab_size = len(words) + 1

print(messages_train[0])
print(x_train[0])

print(messages_test[0])
print(x_test[0])
vocab_size

#padding 

from keras.preprocessing.sequence import pad_sequences

maxlen = 100

x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)
x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)

print(messages_train[0])
print(x_train[0])

"""# Creating embedding layer using Keras tutorial"""

my_embeddings = {}
#GloVe file stored in 'we' at beginning of notebook
f = open(we)
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    my_embeddings[word] = coefs
f.close()

print('Found %s word vectors.' % len(my_embeddings))

my_matrix = np.zeros((vocab_size, 100))
for word, w in words.items():
    embedding_vector = my_embeddings.get(word)
    if embedding_vector is not None:
        my_matrix[w] = embedding_vector

my_embeddinglayer = Embedding(vocab_size,100,weights=[my_matrix],input_length=maxlen,trainable=False)

"""#Build the CNN model"""

def my_cnn():
    #begin model
    model = Sequential()

    #add layers
    model.add(my_embeddinglayer) 
    model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu')) 
    model.add(Dropout(0.5)) 
    model.add(layers.GlobalMaxPooling1D()) 
    model.add(Dropout(0.5)) 
    model.add(layers.Dense(3, activation='softmax'))

    #compile model
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()
    return model

model = my_cnn()
training = model.fit(x_train, y_train, epochs=15, verbose=1, validation_split=0.1, batch_size=32)

"""#Evaluation"""

loss, accuracy = model.evaluate(x_train, y_train, verbose=1)
print('Training Accuracy:  {:.2f}'. format(accuracy))

loss, accuracy = model.evaluate(x_train, y_train, verbose=1)
print('Training Loss:  {:.2f}'. format(loss))

loss, accuracy = model.evaluate(x_test, y_test, verbose=1)
print('Validation Accuracy:  {:.2f}'. format(accuracy))

loss, accuracy = model.evaluate(x_test, y_test, verbose=1)
print('Validation Loss:  {:.2f}'. format(loss))

import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(training):
    acc = training.history['acc']
    val_acc = training.history['val_acc']
    loss = training.history['loss']
    val_loss = training.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(14, 8))
    plt.subplot(1, 2, 2)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'g', label='Training loss')
    plt.plot(x, val_loss, 'y', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

plot_history(training)

#change shape and set variables
y_pred = np.argmax(y_test, axis=-1)
y_true = ratings

#confusion matrix

from sklearn.metrics import confusion_matrix
y_true = pd.Series(y_true, name='True')
y_pred = pd.Series(y_pred, name='Predicted')

df_confusion = pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

df_confusion